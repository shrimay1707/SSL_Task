{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht-id979rglZ"
      },
      "outputs": [],
      "source": [
        "#Importing most necessary libraries here\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ZilcEnrglc"
      },
      "source": [
        "Defining Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGsc7Noyrgle",
        "outputId": "d968b6b5-3394-436e-bc66-b5a26f65f6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzV68emHrgle",
        "outputId": "a8fe37ae-edde-4879-8a53-4c760b36d797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRuMfpwQv3Qu",
        "outputId": "d042440c-93f4-4e31-cc57-009b3d2c272b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision timm\n",
        "from timm import create_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWuKa9mIrglg",
        "outputId": "b0cf5ea5-9f58-4154-d47e-079b89eac79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "total_size = len(cifar_dataset)\n",
        "pretrain_size = int(0.05 * total_size)\n",
        "finetune_size = int(0.45 * total_size)\n",
        "test_size = total_size - pretrain_size - finetune_size\n",
        "\n",
        "# Split dataset into 5% for pre-training, 45% for fine-tuning, and 50% for testing, as specified in the task.\n",
        "pretrain_dataset, remaining_dataset = random_split(cifar_dataset, [pretrain_size, total_size - pretrain_size])\n",
        "finetune_dataset, test_dataset = random_split(remaining_dataset, [finetune_size, test_size])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypEm_fP8ls07"
      },
      "source": [
        " To understand the effect  of supervised learning and pre-training, we need to compare the performance of the model at various stages.\n",
        "\n",
        " First of all, let us test the base capacity of our model. Let us see it's test accuracy without training it and see. This gives us an idea about the capability of the model in it's raw state and helps us compare with our results later. I have named this model \"**model_empty**\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IfgOgTjvbQN"
      },
      "outputs": [],
      "source": [
        "model_empty = create_model('efficientnet_b6', pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOmuaqAiwVpQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohw1Ghmy9-4f"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eJ_xhaDvV8L"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model_empty(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert the lists to NumPy arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW1YEW8v0RRe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "predictions_np = np.array(predictions)\n",
        "true_labels_np = np.array(true_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_untrained = accuracy_score(true_labels_np, predictions_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgHt2txdwf9R",
        "outputId": "dc4f825c-7e6d-427d-de86-582a251c25d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for completely untrained model: 0.02800%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f'Test Accuracy for completely untrained model: {accuracy_untrained* 100:.5f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On testing untrained model, we see an accuracy of 0.02%. This low accuracy is expected since the model hasn't been trained and has nt learned any weights. Moreover, since we have kept the pretrained parameter set to **False**, the model is initialised without any pre-trained weights."
      ],
      "metadata": {
        "id": "e2bNVU6hQjIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9b7fDZr2G4o"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgNUJpR-2xIv",
        "outputId": "029e9f4b-9c4b-4536-ae60-b9e0c5e913fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing, but no pre-training\n",
        "Here, we will Train our model on the 45%-finetune dataset, and test it on the remaining 50%. However, no pretraining will be done.\n",
        "\n",
        "EfficientNet model is being used."
      ],
      "metadata": {
        "id": "shk9IZCpVtLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwOFeOLx27mc"
      },
      "outputs": [],
      "source": [
        "\n",
        "pretrain_loader = torch.utils.data.DataLoader(pretrain_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "finetune_loader = torch.utils.data.DataLoader(finetune_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trtbRkX4-M2O",
        "outputId": "de5b67c7-0a17-4871-dfec-f62333615e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b6\n"
          ]
        }
      ],
      "source": [
        "from tqdm import trange\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Specify the EfficientNet model (e.g., EfficientNet-B0)\n",
        "model = EfficientNet.from_pretrained('efficientnet-b6', num_classes=10)  # 10 classes in CIFAR-10\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All experiments were initially carried out on my local PC using VSCode, however, were later shifted to Google Colab to gain GPU access."
      ],
      "metadata": {
        "id": "JU9S_oWyRa9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFDND-qfrUiR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXRyNdJR2k7W",
        "outputId": "c96b69f6-4dd9-4d82-b461-935347d98e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [03:42<00:00, 44.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.80684\n"
          ]
        }
      ],
      "source": [
        "\n",
        "num_epochs = 5\n",
        "model = model.to(device)\n",
        "\n",
        "# Training loop on the train dataset\n",
        "for epoch in trange(num_epochs):\n",
        "    for inputs, labels in finetune_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation on the test dataset\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move inputs and labels to the same device as the model\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zccLikBx9Yg2"
      },
      "source": [
        "Here we see the accuracy of the model on being trained on the 45% finetune dataset, and tested on the 50% test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWbeYhqGVCIl",
        "outputId": "0a8b8ff1-4d3e-40d2-b2bd-9a7950fc7159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.80684\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move inputs and labels to the same device as the model\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbj1KlRu8HuD",
        "outputId": "31564259-468c-46a4-ea23-aace50023059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 80.68%\n"
          ]
        }
      ],
      "source": [
        "accuracy_percentage_nopre = accuracy * 100\n",
        "print(f'Test Accuracy: {accuracy_percentage_nopre:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy seen above is on training the model on 45% of the dataset and then testing on the remaining 50%. We can see the significant, and obvious, increase in accuracy, considering that the model has been trained on correct data."
      ],
      "metadata": {
        "id": "DZN-eiK_Rfi3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHpQDMSRZNNS"
      },
      "source": [
        "### Self-supervised Learning task.\n",
        "\n",
        "Below, we start the jigsaw puzzle transformation. In the reference provided by the sustainability lab, it was suggested to segment the image into 9 pieces, but I have gone with a 4-piece, i.e, a 2x2 grid, in the interest of time. In the first code block, we are simply dividing the image into 4 parts, and saving every permutation of the shuffled images, alongside their labels, which will help in performing the self-supervised task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkzNIK_Mrglh"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "bb = []\n",
        "dd = []\n",
        "label = [0,1,2,3]\n",
        "label_permute = list(itertools.permutations(label))\n",
        "h = 32\n",
        "w = 32\n",
        "for img, _ in pretrain_dataset:\n",
        "\n",
        "        c, w, h = img.shape\n",
        "        img_part = np.zeros((4, 3, h // 2, w // 2))\n",
        "        img_part[0] = img[:, : h // 2, : w // 2]\n",
        "        img_part[1] = img[:, h // 2 :, : w // 2]\n",
        "        img_part[2] = img[:, : h // 2, w // 2 :]\n",
        "        img_part[3] = img[:, h // 2 :, w // 2 :]\n",
        "\n",
        "        for j,k in zip(label_permute, range(len(label_permute))):\n",
        "            img_up = np.concatenate((img_part[j[0]], img_part[j[2]]), axis=2)\n",
        "        # Combine the bottom-left and bottom-right parts\n",
        "            img_down = np.concatenate((img_part[j[1]], img_part[j[3]]), axis=2)\n",
        "\n",
        "        # Combine the top and bottom halves to create the final stitched image\n",
        "            remade_image = np.concatenate((img_up, img_down), axis=1)\n",
        "            pil_image = Image.fromarray((remade_image * 255).astype(np.uint8).transpose(1, 2, 0))\n",
        "\n",
        "            bb.append(torch.tensor(remade_image, dtype=torch.float32))\n",
        "            dd.append(torch.tensor(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-kbu6o3rglh",
        "outputId": "3d577fdf-b001-4598-8990-9f0e9f6272a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "len(dd)\n",
        "#should be 60000(2500x24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA9UPvYXKO42"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DfJotoAJr1O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Apply transformations if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Assuming you have lists 'image_list' and 'label_list'\n",
        "# You can define a transform if needed (e.g., convert PIL image to tensor)\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Create a CustomDataset\n",
        "custom_dataset = CustomDataset(images=bb, labels=dd, transform=None)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 128\n",
        "ssl_data_loader = DataLoader(dataset=custom_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaQlVazHjcuc",
        "outputId": "f42be495-2e93-4fca-e8ad-0ee6cbaa5d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6t7viYM48Gg",
        "outputId": "0994bde2-a0f6-410d-eb6b-120035bc1211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b6\n"
          ]
        }
      ],
      "source": [
        "from tqdm import trange\n",
        "import tqdm\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Specify the EfficientNet model_ssl (e.g., EfficientNet-B0)\n",
        "model_ssl = EfficientNet.from_pretrained('efficientnet-b6', num_classes=24)  # 10 classes in CIFAR-10\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_ssl.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "model_ssl = model_ssl.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Only SSL, No finetuning"
      ],
      "metadata": {
        "id": "7pqqIsx8WTbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcd9cogj5BcU",
        "outputId": "9bc9e71d-bdf0-4eb4-a24f-2dbdda62b796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [01:57<17:40, 117.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3279, Accuracy: 0.5850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [03:54<15:39, 117.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 0.1462, Accuracy: 0.9564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [05:51<13:39, 117.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 0.0564, Accuracy: 0.9843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [07:48<11:42, 117.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 0.0421, Accuracy: 0.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [09:46<09:46, 117.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 0.0331, Accuracy: 0.9913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [11:43<07:49, 117.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 0.0294, Accuracy: 0.9922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [13:39<05:50, 116.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 0.0284, Accuracy: 0.9930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [15:36<03:53, 116.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 0.0284, Accuracy: 0.9932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [17:33<01:56, 116.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 0.0445, Accuracy: 0.9892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [19:29<00:00, 116.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 0.0294, Accuracy: 0.9931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training loop on the train dataset\n",
        "for epoch in trange(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    for inputs, labels in ssl_data_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_ssl(inputs.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels.to(device)).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate epoch-level metrics\n",
        "    epoch_loss = total_loss / len(ssl_data_loader)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    # Display loss and accuracy after each epoch\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWGATeFThaAF",
        "outputId": "7f4d8b3a-0338-4f1c-fdc3-b7c7ad634af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 5.02400%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Evaluation on the test dataset\n",
        "model_ssl.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model_ssl(inputs.to(device))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "accuracy_percentage = accuracy * 100\n",
        "print(f'Test Accuracy: {accuracy_percentage:.5f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL1JzwdEOzES"
      },
      "source": [
        "\n",
        "The accuracy obtained above (around 5%) is for when the SSL task is carried out followed by testing, nothing else. This low value is expected because we haven't trained it on a substantial amount of data. Moreover, we are training it on shuffled images to predict the permutation, and testing it by making it predict the correct class from the CIFAR-10 dataset.\n",
        "\n",
        "However, we can see that this is a **significant increase from the 0.02% accuracy** we recorded on the completely untrained model, which shows that **valuable feature recognition and image patterns** are learnt during the SSL task, and are useful for the downstream classification task.\n",
        "\n",
        "Now below, we will add a linear layer to make our model suitable for CIFAR-10 classification task.The model will carry forward the weights it has learned from the SSL task, and be further finetuned by training on the 45% finetune dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izv49SmtOweO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Load pre-trained EfficientNet model\n",
        "efficientnet_model = model_ssl\n",
        "# Get the in_features of the last layer in the EfficientNet model\n",
        "in_features = efficientnet_model._fc.in_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oJku2ZUavaO"
      },
      "source": [
        "Below we will define the architecture we want to replace the last layer of our EffNet architecture with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdoHqiaBO5Zk"
      },
      "outputs": [],
      "source": [
        "custom_model = nn.Sequential(\n",
        "    nn.Linear(in_features=2304, out_features=512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=512, out_features=10),  # Adjust out_features based on your requirement\n",
        "    nn.Softmax(dim=1)  # Add softmax activation along dimension 1 (assuming a classification task)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrgX6Qc9ociy"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "efficientnet_model.to(device)\n",
        "custom_model.to(device)\n",
        "\n",
        "efficientnet_model._fc = nn.Linear(in_features=2304, out_features=10).to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4e9YEDvBAfP"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use our modified model for the CIFAR-10 classification task below."
      ],
      "metadata": {
        "id": "WrIarQahWaec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7tXG8C2PfkH",
        "outputId": "788c517c-c56f-4197-8b74-000074d35a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  20%|██        | 1/5 [00:46<03:06, 46.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.0188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:  40%|████      | 2/5 [01:33<02:20, 46.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Loss: 0.0188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:  60%|██████    | 3/5 [02:20<01:33, 46.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Loss: 0.0188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:  80%|████████  | 4/5 [03:07<00:46, 46.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Loss: 0.0188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs: 100%|██████████| 5/5 [03:53<00:00, 46.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Loss: 0.0188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.09708\n"
          ]
        }
      ],
      "source": [
        "\n",
        "num_epochs = 5\n",
        "model = model.to(device)\n",
        "\n",
        "# Training loop on the train dataset\n",
        "for epoch in trange(num_epochs, desc='Epochs'):\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in finetune_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = efficientnet_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}')\n",
        "\n",
        "\n",
        "# Evaluation on the test dataset\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move inputs and labels to the same device as the model\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = efficientnet_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmXxtjASooLt",
        "outputId": "6e55bdbd-4e32-4e70-ae92-4451b5225be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 9.708\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Evaluation on the test dataset\n",
        "efficientnet_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = efficientnet_model(inputs.to(device))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy*100}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PzLzJZrfJQV",
        "outputId": "3194f83d-d03e-4374-b54d-254d63d700a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 9.708\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f'Test Accuracy: {accuracy*100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On finetuning the model on our finetune dataset, we can see that the accuracy has increased from 5% to 9.7%. This increase can be seen since the model has since been trained on the CIFAR-10 dataset to predict the correct class from the image.\n",
        "\n",
        "However, I was expecting the model to outperform the experiment where training is done without the SSL pretraining, but unfortunately, I couldn't achieve those results. I tried various debugging statements, added print statements in various places, etc to debug the problem, but wasn't succesful. I even plotted some figures, including the confusion matrix, the Loss vs Epochs graph, etc, but couldn't reach the root of the problem.\n",
        "\n",
        "I hope to learn in thourough depth once I join the team at the Sustainability lab :)"
      ],
      "metadata": {
        "id": "Lc_hrxmDWgb-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "M5EunbQbeFa5",
        "outputId": "c48f15a6-2e21-4ddc-869a-30966b751b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix:\n",
            "[[1 0 0 0 0 2 0 0 0 0]\n",
            " [1 0 0 0 0 1 0 0 0 0]\n",
            " [1 0 0 1 0 3 0 0 0 0]\n",
            " [4 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 1 0 1 0 0 0 0]\n",
            " [2 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 5 0 0 0 0]\n",
            " [3 0 0 0 1 2 0 0 0 0]\n",
            " [2 0 0 1 1 1 0 0 0 0]\n",
            " [2 0 0 0 0 1 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_matrix = confusion_matrix(labels.cpu().numpy(), predicted.cpu().numpy())\n",
        "print(f'\\nConfusion Matrix:\\n{conf_matrix}')\n",
        "\n",
        "# Display the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discarded Codes\n",
        "Below are several app"
      ],
      "metadata": {
        "id": "_BR3FjsUXh7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQaRs_jrBFSh"
      },
      "outputs": [],
      "source": [
        "# # Training loop on the train dataset\n",
        "# for epoch in trange(num_epochs):\n",
        "#     for inputs, labels in finetune_loader:\n",
        "#         print(inputs.shape)\n",
        "#         print(labels.shape)\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(inputs.to(device))\n",
        "\n",
        "#         # Add a singleton dimension to labels\n",
        "#         labels = labels.unsqueeze(1)\n",
        "#         labels = labels.unsqueeze(1)\n",
        "\n",
        "#         loss = criterion(outputs, labels.to(device))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Calculate accuracy\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         correct_predictions += (predicted == labels.to(device)).sum().item()\n",
        "#         total_samples += labels.size(0)\n",
        "\n",
        "#     # Calculate epoch-level metrics\n",
        "#     epoch_loss = total_loss / len(finetune_loader)  # Fix the loader name\n",
        "#     epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "#     # Display loss and accuracy after each epoch\n",
        "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oaesg7ZCrglh"
      },
      "outputs": [],
      "source": [
        "# from tqdm import trange\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(cfn_model.parameters(), lr=0.0001)\n",
        "# # pretrain_loader = tqdm(pretrain_loader, total=len(pretrain_loader), desc=\"Pretraining\")\n",
        "# print(100)\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Assuming cfn_model, criterion, optimizer, and data (bb, dd) are defined as before\n",
        "\n",
        "# cfn_model.train()\n",
        "# num_pretrain_epochs = 25\n",
        "# losses = []  # To store losses for plotting\n",
        "\n",
        "# for epoch in trange(num_pretrain_epochs):\n",
        "#     epoch_losses = []\n",
        "#     for inputs, label in zip(bb, dd):\n",
        "#         optimizer.zero_grad()\n",
        "#         inputs = inputs.unsqueeze(0)\n",
        "#         # print(inputs.shape)\n",
        "#         # print(label.shape)\n",
        "#         # inputs = inputs.mean(dim=1, keepdim=True)\n",
        "#         # inputs = inputs.repeat(1, 3, 1, 1)\n",
        "#         outputs = cfn_model(inputs)\n",
        "#         if not torch.is_tensor(label):\n",
        "#             label = torch.tensor(label, dtype=torch.long)\n",
        "#         label = label.reshape(1)\n",
        "#         loss = criterion(outputs, label)\n",
        "#         epoch_losses.append(loss.item())\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Calculate average loss for the epoch\n",
        "#     print(epoch_losses)\n",
        "#     avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "#     losses.append(avg_epoch_loss)\n",
        "\n",
        "#     print(f\"Epoch: {epoch}, Avg Loss: {avg_epoch_loss}\")\n",
        "\n",
        "# # Plot loss vs epoch\n",
        "# plt.plot(range(1, num_pretrain_epochs + 1), losses, marker='o')\n",
        "# plt.title('Loss vs Epoch')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Average Loss')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wjDtzmlrglf"
      },
      "outputs": [],
      "source": [
        "# from tqdm import trange\n",
        "# class JigsawPuzzleTransform:\n",
        "#     def __init__(self, grid_size=1):\n",
        "#         self.grid_size = grid_size\n",
        "\n",
        "#     def __call__(self, img):\n",
        "#         # Assuming img is a PIL Image\n",
        "#         w, h = img.size\n",
        "#         patch_size = w // self.grid_size\n",
        "\n",
        "#         patches = []\n",
        "#         for i in trange(self.grid_size):\n",
        "#             for j in range(self.grid_size):\n",
        "#                 patch = transforms.functional.crop(img, i * patch_size, j * patch_size, patch_size, patch_size)\n",
        "#                 patches.append(patch)\n",
        "\n",
        "#         # Shuffle the patches\n",
        "#         random.shuffle(patches)\n",
        "\n",
        "#         # Concatenate the shuffled patches\n",
        "#         shuffled_img = torch.cat(patches, dim=2)\n",
        "\n",
        "#         return shuffled_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YCrNpmmrglg"
      },
      "outputs": [],
      "source": [
        "# bb = []\n",
        "# for img, _ in pretrain_dataset:\n",
        "#     # Assuming img is a tensor with shape (channels, height, width)\n",
        "#     c, w, h = img.shape\n",
        "#     grid_size = 2\n",
        "#     patch_size = w // grid_size\n",
        "\n",
        "#     patches = []\n",
        "#     for i in range(grid_size):\n",
        "#         for j in range(grid_size):\n",
        "#             patch = img[:, i * patch_size:(i + 1) * patch_size, j * patch_size:(j + 1) * patch_size]\n",
        "#             patches.append(patch)\n",
        "\n",
        "#     # Shuffle the patches\n",
        "\n",
        "#     random.shuffle(patches)\n",
        "\n",
        "#     # Concatenate the shuffled patches along the last two dimensions (width and height)\n",
        "#     shuffled_img = torch.cat(patches, dim=1)\n",
        "\n",
        "#     bb.append(shuffled_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWaUZZHErglf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class CFN(nn.Module):\n",
        "#     def __init__(self, num_permutations):\n",
        "#         super(CFN, self).__init__()\n",
        "\n",
        "#         # Define CFN architecture\n",
        "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "#         self.fc1 = nn.Linear(128 * 8 * 8, num_permutations)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Implement forward pass for self-supervised task\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = x.view(-1, 128 * 8 * 8)  # Flatten the output for fully connected layer\n",
        "#         x = self.fc1(x)\n",
        "#         x = F.softmax(x, dim=1)\n",
        "#         return x\n",
        "\n",
        "# # Set up CFN model for self-supervised pre-training\n",
        "# num_permutations = 24\n",
        "# cfn_model = CFN(num_permutations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTMoCaCuzTzs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk2_qL8uFyY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc2e42e-b002-4066-913d-c5c16ce5284b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "len(bb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCXpTDIPrglh"
      },
      "outputs": [],
      "source": [
        "# pretrain_loader_iter = iter(finetune_loader)\n",
        "# inputs, _ = next(pretrain_loader_iter)\n",
        "# print(inputs.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g0CyxNH0w0G"
      },
      "outputs": [],
      "source": [
        "# model_effnet = create_model('efficientnet_b6', pretrained=False)\n",
        "# num_classes = 10  # Adjust according to your task\n",
        "# model_effnet.fc = nn.Linear(model_effnet.num_features, num_classes)\n",
        "\n",
        "# # Define loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model_effnet.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop\n",
        "# num_epochs = 5  # Adjust as needed\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToPILImage(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "# model_effnet.train()  # Set the model_effnet to training mode\n",
        "# for epoch in range(num_epochs):\n",
        "#     for inputs, labels in finetune_dataset:\n",
        "#         # Apply transforms and add batch dimension\n",
        "#         inputs = transform(inputs).unsqueeze(0)\n",
        "\n",
        "#         optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model_effnet(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv1QTHD4EQYz"
      },
      "outputs": [],
      "source": [
        "# from torchvision import transforms\n",
        "\n",
        "# # Assuming 'efficient_net_model' is your EfficientNet model\n",
        "\n",
        "# # Define a transform for EfficientNet\n",
        "# efficientnet_transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # Assuming EfficientNet is trained on 224x224 images\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# cfn_model.train()\n",
        "# num_pretrain_epochs = 25\n",
        "# losses = []  # To store losses for plotting\n",
        "\n",
        "# for epoch in trange(num_pretrain_epochs):\n",
        "#     epoch_losses = []\n",
        "#     for inputs, label in zip(bb, dd):\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Convert PyTorch tensor to PIL Image\n",
        "#         inputs_pil = transforms.ToPILImage()(inputs)\n",
        "\n",
        "#         # Apply EfficientNet transform\n",
        "#         inputs_transformed = efficientnet_transform(inputs_pil)\n",
        "\n",
        "#         # Add a batch dimension\n",
        "#         inputs_transformed = inputs_transformed.unsqueeze(0)\n",
        "\n",
        "#         # Make sure the label is a tensor\n",
        "#         if not torch.is_tensor(label):\n",
        "#             label = torch.tensor(label, dtype=torch.long)\n",
        "#         label = label.reshape(1)\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = cfn_model(inputs_transformed)\n",
        "\n",
        "#         # Calculate loss and perform backward pass\n",
        "#         loss = criterion(outputs, label)\n",
        "#         epoch_losses.append(loss.item())\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Calculate average loss for the epoch\n",
        "#     avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "#     losses.append(avg_epoch_loss)\n",
        "\n",
        "#     print(f\"Epoch: {epoch}, Avg Loss: {avg_epoch_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfmiAC8qrgli"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCXxIUW6rgli"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(cfn_model.parameters(), lr=0.001)\n",
        "# pretrain_loader = tqdm(pretrain_loader, total=len(pretrain_loader), desc=\"Pretraining\")\n",
        "# print(100)\n",
        "# cfn_model.train()\n",
        "# num_pretrain_epochs = 2\n",
        "# print(0)\n",
        "# for epoch in range(num_pretrain_epochs):\n",
        "#     print(3)\n",
        "#     for inputs, _ in pretrain_loader:\n",
        "#         print(4)\n",
        "#         optimizer.zero_grad()\n",
        "#         print(1)\n",
        "#         outputs = cfn_model(inputs)\n",
        "#         target = torch.arange(num_permutations)[:inputs.size(0)]\n",
        "#         print(2)\n",
        "#         loss = criterion(outputs, target)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Update the progress bar\n",
        "#         pretrain_loader.set_postfix({\"Epoch\": epoch + 1, \"Loss\": loss.item()}, refresh=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq0bpdmC-_2q"
      },
      "outputs": [],
      "source": [
        "# cfn_model = EfficientNet.from_pretrained('efficientnet-b6', num_classes=24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vebhzk9urgli"
      },
      "outputs": [],
      "source": [
        "# classifier_model = nn.Sequential(\n",
        "#     cfn_model,  # Use the pre-trained CFN as the initial layers\n",
        "#     nn.Linear(num_permutations, 10)  # Adjust num_classes as needed\n",
        "# )\n",
        "\n",
        "# classifier_criterion = nn.CrossEntropyLoss()\n",
        "# classifier_optimizer = optim.Adam(classifier_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbrRvK2Qrgli"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# num_fine_tune_epochs = 5\n",
        "\n",
        "\n",
        "# for epoch in trange(num_fine_tune_epochs):\n",
        "#     classifier_model.train()\n",
        "\n",
        "#     for inputs, labels in finetune_dataset:\n",
        "#         optimizer.zero_grad()\n",
        "#         classifier_outputs = classifier_model(inputs)\n",
        "#         # Convert labels to tensor if not already\n",
        "#         if not torch.is_tensor(labels):\n",
        "#             labels = torch.tensor(labels, dtype=torch.long)\n",
        "#         labels = labels.reshape(1)\n",
        "#         # Compute cross-entropy loss\n",
        "#         loss = F.cross_entropy(classifier_outputs, labels)\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9HU8L53rgli"
      },
      "outputs": [],
      "source": [
        "# classifier_model.eval()\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for inputs, labels in test_dataset:\n",
        "#         # Convert labels to tensor if not already\n",
        "#         if not torch.is_tensor(labels):\n",
        "#             labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "#         outputs = classifier_model(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "#         # Ensure that labels is a tensor\n",
        "#         labels = torch.tensor(labels, dtype=torch.long)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# accuracy = correct / len(test_dataset)\n",
        "# print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}